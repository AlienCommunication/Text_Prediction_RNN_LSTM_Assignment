{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baby Name generator on chracter level\n",
    "\n",
    "### Reference \n",
    "\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=6ORnRAz3gnA&t=59s\"> YouTube Channel on LSTM in keras</a>\n",
    "\n",
    "<a href=\"https://keras.io/api/layers/recurrent_layers/lstm/\"> Keras official documentation</a>             \n",
    "\n",
    "<a href=\"https://kite.com/python/docs/keras.layers.LSTM\">Kite LSTM Documentation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "**Recurrent Neural Networks**  - suffer from short-term memory. If a sequence is long enough, they’ll have a hard time carrying information from earlier time steps to later ones. So if you are trying to process a paragraph of text to do predictions, RNN’s may leave out important information from the beginning.\n",
    "During back propagation, recurrent neural networks suffer from the vanishing gradient problem.\n",
    "\n",
    "\n",
    "# LSTM\n",
    "\n",
    "*LSTM stands for long short term memory. It is a model or architecture that extends the memory of recurrent neural networks. Typically, recurrent neural networks have ‘short term memory’ in that they use persistent previous information to be used in the current neural network. Essentially, the previous information is used in the present task. That means we do not have a list of all of the previous information available for the neural node*\n",
    "\n",
    "**How does it work**\n",
    "\n",
    "*LSTM introduces long-term memory into recurrent neural networks. It mitigates the vanishing gradient problem, which is where the neural network stops learning because the updates to the various weights within a given neural network become smaller and smaller. It does this by using a series of ‘gates’. These are contained in memory blocks which are connected through layers, like this*\n",
    "\n",
    "\n",
    "                                          \n",
    "**A simple LSTM model consist four gates in it**\n",
    "\n",
    "**Forget Gate:**\n",
    "After getting the output of previous state, h(t-1), Forget gate helps us to take decisions about what must be removed from h(t-1) state and thus keeping only relevant stuff. It is surrounded by a sigmoid function which helps to crush the input between [0,1].\n",
    "\n",
    "**Input Gate:**\n",
    "\n",
    "In the input gate, we decide to add new stuff from the present input to our present cell state scaled by how much we wish to add them.\n",
    "\n",
    "\n",
    "**Output Gate:** \n",
    "Finally we’ll decide what to output from our cell state which will be done by our sigmoid function.\n",
    "\n",
    "\n",
    "**LSTM is divided into six parts**\n",
    "\n",
    "**Vanilla LSTM** - A Vanilla LSTM is an LSTM model that has a single hidden layer of LSTM units, and an output layer used to make a prediction.\n",
    "\n",
    "**Stacked LSTM** -  Multiple hidden LSTM layers can be stacked one on top of another in what is referred to as a Stacked LSTM model.\n",
    "An LSTM layer requires a three-dimensional input and LSTMs by default will produce a two-dimensional output as an interpretation from the end of the sequence.\n",
    "\n",
    "**Bidirectional LSTM** -  On some sequence prediction problems, it can be beneficial to allow the LSTM model to learn the input sequence both forward and backwards and concatenate both interpretations.We can implement a Bidirectional LSTM for univariate time series forecasting by wrapping the first hidden layer in a wrapper layer called Bidirectional.\n",
    "\n",
    "**CNN LSTM** - A convolutional neural network, or CNN for short, is a type of neural network developed for working with two-dimensional image data.\n",
    "The CNN can be very effective at automatically extracting and learning features from one-dimensional sequence data such as univariate time series data.\n",
    "A CNN model can be used in a hybrid model with an LSTM backend where the CNN is used to interpret subsequences of input that together are provided as a sequence to an LSTM model to interpret. This hybrid model is called a CNN-LSTM.\n",
    "\n",
    "**ConvLSTM** - A type of LSTM related to the CNN-LSTM is the ConvLSTM, where the convolutional reading of input is built directly into each LSTM unit.\n",
    "The ConvLSTM was developed for reading two-dimensional spatial-temporal data, but can be adapted for use with univariate time series forecasting.\n",
    "The layer expects input as a sequence of two-dimensional images, therefore the shape of input data must be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('baby-names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(258000, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\tJohn</td>\n",
       "      <td>John\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\tWilliam</td>\n",
       "      <td>William\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\tJames</td>\n",
       "      <td>James\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\tCharles</td>\n",
       "      <td>Charles\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\tGeorge</td>\n",
       "      <td>George\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name     target\n",
       "0     \\tJohn     John\\n\n",
       "1  \\tWilliam  William\\n\n",
       "2    \\tJames    James\\n\n",
       "3  \\tCharles  Charles\\n\n",
       "4   \\tGeorge   George\\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "6777    False\n",
       "6778    False\n",
       "6779    False\n",
       "6780    False\n",
       "6781    False\n",
       "Length: 6782, dtype: bool"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding duplicate rows\n",
    "\n",
    "data.duplicated()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I create a dataframe with all the unique words and create the vocab of char out of those words. Further since my target is the next char, I will shift my target by one timestamp and append ‘\\n’ to represent that word ended here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe with unique words\n",
    "data=pd.DataFrame({'name':data.name.unique()})\n",
    "\n",
    "#Shifting my target by one timestamp t  because Target is the next char\n",
    "\n",
    "data['name']=data.name.apply(lambda x:'\\t'+x)\n",
    "\n",
    "#Appending '\\n' to represent that word ended here'\n",
    "data['target']=data.name.apply(lambda x:x[1:len(x)]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocab dict\n",
    "all_chars=set()\n",
    "for name in data.name:\n",
    "    for c in name:\n",
    "        if c not in all_chars:\n",
    "            all_chars.add(c)\n",
    "all_chars.add('\\n')\n",
    "\n",
    "# max length of a name is 11\n",
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(all_chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(all_chars)) }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Inputs are 3D numpy matrix of size which accepts training datasize, max length of names, chracter of vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here giving  training datasize, length of name and vocabulary size as 54 with datatype\n",
    "\n",
    "lenght_list=[]\n",
    "for l in data.name:\n",
    "    lenght_list.append(len(l))\n",
    "max_len = np.max(lenght_list)\n",
    "input_data = np.zeros((len(data.name), max_len, 54),dtype='float32')\n",
    "output_data = np.zeros((len(data.name), max_len, 54),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate input and output data\n",
    "for i, x in enumerate(data.name):\n",
    "    for t, ch in enumerate(x):\n",
    "        input_data[i, t, char_to_ix[ch]] = 1.\n",
    "for i, x in enumerate(data.target):\n",
    "    for t, ch in enumerate(x):\n",
    "        output_data[i,t, char_to_ix[ch]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Description: \n",
    "\n",
    "**1. TimeDistributed** - Is used to keep one-to-one relations on input and output. Let's say you have 60 time steps with 100 samples of data (60 x 100 in another word) and you want to use RNN/LSTM with output of 200. If you don't use timedistributed dense layer, you will get 100 x 60 x 200 tensor. So you have the output flattened with each timestep mixed. If you apply the timedistributed dense, you are going to apply fully connected dense on each time step and get output separately by timesteps.\n",
    "\n",
    "**2. RMSprop**- Is used as an optimizer\n",
    "\n",
    "**3. Parameter \"Return_Sequences\"** in first layer (LSTM), If the return_sequences is set to False in Keras RNN layers, this means the RNN layer will only return the last hidden state output. But here I am setting it as True hence won't return the only Last hidden state output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(max_len, len(all_chars)), return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(len(all_chars))))\n",
    "model.add(TimeDistributed(Activation('softmax')))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 12, 50)            21000     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 12, 54)            2754      \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 12, 54)            0         \n",
      "=================================================================\n",
      "Total params: 23,754\n",
      "Trainable params: 23,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing  Sample function to generate new names**\n",
    "def onend(epoch, logs):\n",
    "    if epoch%2==0 and epoch !=0:\n",
    "        print('----- Generating text after Epoch: %d' % epoch)\n",
    "        for i in range(0,10):\n",
    "            stop=False\n",
    "            ch='\\t'\n",
    "            counter=1\n",
    "            target_seq = np.zeros((1, max_len, 54))\n",
    "            target_seq[0, 0, char_to_ix[ch]] = 1.\n",
    "            while stop == False and counter < 10:\n",
    "                #sample the data\n",
    "                probs = model.predict_proba(target_seq, verbose=0)[:,counter-1,:]\n",
    "                c= np.random.choice(sorted(list(all_chars)), replace =False,p=probs.reshape(54))\n",
    "                #c=ix_to_char[np.argmax(probs.reshape(28))]\n",
    "                if c=='\\n':\n",
    "                    stop=True\n",
    "                else:\n",
    "                    ch=ch+c\n",
    "                    target_seq[0,counter , char_to_ix[c]] = 1.\n",
    "                    counter=counter+1\n",
    "            print(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6782/6782 [==============================] - 9s 1ms/step - loss: 0.9399\n",
      "Epoch 2/50\n",
      "6782/6782 [==============================] - 8s 1ms/step - loss: 0.9390\n",
      "Epoch 3/50\n",
      "6782/6782 [==============================] - 9s 1ms/step - loss: 0.9373\n",
      "----- Generating text after Epoch: 2\n",
      "\tAlbreda\n",
      "\tSone\n",
      "\tIsham\n",
      "\tAnder\n",
      "\tGennie\n",
      "\tBuzle\n",
      "\tViola\n",
      "\tShetta\n",
      "\tMealisa\n",
      "\tFrona\n",
      "Epoch 4/50\n",
      "6782/6782 [==============================] - 8s 1ms/step - loss: 0.9373\n",
      "Epoch 5/50\n",
      "6782/6782 [==============================] - 9s 1ms/step - loss: 0.9362\n",
      "----- Generating text after Epoch: 4\n",
      "\tArah\n",
      "\tFricly\n",
      "\tDannielle\n",
      "\tEus\n",
      "\tJavion\n",
      "\tLetal\n",
      "\tMinnit\n",
      "\tErna\n",
      "\tNichole\n",
      "\tAlisha\n",
      "Epoch 6/50\n",
      "6782/6782 [==============================] - 10s 1ms/step - loss: 0.9361\n",
      "Epoch 7/50\n",
      "6782/6782 [==============================] - 11s 2ms/step - loss: 0.9361\n",
      "----- Generating text after Epoch: 6\n",
      "\tBarretta\n",
      "\tLouis\n",
      "\tSon\n",
      "\tAlford\n",
      "\tMaxulla\n",
      "\tTreitha\n",
      "\tFedy\n",
      "\tHaylee\n",
      "\tRomona\n",
      "\tQueea\n",
      "Epoch 8/50\n",
      "6782/6782 [==============================] - 12s 2ms/step - loss: 0.9354\n",
      "Epoch 9/50\n",
      "6782/6782 [==============================] - 12s 2ms/step - loss: 0.9349\n",
      "----- Generating text after Epoch: 8\n",
      "\tDana\n",
      "\tRovon\n",
      "\tEwing\n",
      "\tTaviona\n",
      "\tWelden\n",
      "\tKimp\n",
      "\tWilla\n",
      "\tDyan\n",
      "\tMicayla\n",
      "\tOren\n",
      "Epoch 10/50\n",
      "6782/6782 [==============================] - 10s 1ms/step - loss: 0.9346\n",
      "Epoch 11/50\n",
      "6782/6782 [==============================] - 12s 2ms/step - loss: 0.9347\n",
      "----- Generating text after Epoch: 10\n",
      "\tJanelle\n",
      "\tSpeckeyli\n",
      "\tIzya\n",
      "\tContine\n",
      "\tYvonne\n",
      "\tDelison\n",
      "\tBenjiman\n",
      "\tMargo\n",
      "\tRaynor\n",
      "\tGeorgia\n",
      "Epoch 12/50\n",
      "6782/6782 [==============================] - 10s 1ms/step - loss: 0.9334\n",
      "Epoch 13/50\n",
      "6782/6782 [==============================] - 10s 1ms/step - loss: 0.9338\n",
      "----- Generating text after Epoch: 12\n",
      "\tRili\n",
      "\tSelan\n",
      "\tErolfa\n",
      "\tKathlyn\n",
      "\tGlinda\n",
      "\tLucian\n",
      "\tLakle\n",
      "\tJearahino\n",
      "\tIsaylai\n",
      "\tDustyn\n",
      "Epoch 14/50\n",
      "6782/6782 [==============================] - 9s 1ms/step - loss: 0.9324\n",
      "Epoch 15/50\n",
      "6782/6782 [==============================] - 10s 1ms/step - loss: 0.9331\n",
      "----- Generating text after Epoch: 14\n",
      "\tPawel\n",
      "\tAdit\n",
      "\tLita\n",
      "\tDaisley\n",
      "\tMariano\n",
      "\tEdd\n",
      "\tColen\n",
      "\tCera\n",
      "\tTrayvon\n",
      "\tAdell\n",
      "Epoch 16/50\n",
      "6782/6782 [==============================] - 12s 2ms/step - loss: 0.9322\n",
      "Epoch 17/50\n",
      "6782/6782 [==============================] - 12s 2ms/step - loss: 0.9320: 0s - loss: \n",
      "----- Generating text after Epoch: 16\n",
      "\tJovan\n",
      "\tCrawendra\n",
      "\tCyrmin\n",
      "\tSamira\n",
      "\tDion\n",
      "\tEyver\n",
      "\tJamich\n",
      "\tElia\n",
      "\tFlory\n",
      "\tNackery\n",
      "Epoch 18/50\n",
      "6782/6782 [==============================] - 10s 1ms/step - loss: 0.9318\n",
      "Epoch 19/50\n",
      "6782/6782 [==============================] - 10s 2ms/step - loss: 0.9303\n",
      "----- Generating text after Epoch: 18\n",
      "\tRostin\n",
      "\tAurt\n",
      "\tOspciom\n",
      "\tKassidy\n",
      "\tLuci\n",
      "\tWood\n",
      "\tLinneth\n",
      "\tLora\n",
      "\tBettien\n",
      "\tShan\n",
      "Epoch 20/50\n",
      "6782/6782 [==============================] - 10s 1ms/step - loss: 0.9306\n",
      "Epoch 21/50\n",
      "6782/6782 [==============================] - 10s 1ms/step - loss: 0.9300\n",
      "----- Generating text after Epoch: 20\n",
      "\tGluva\n",
      "\tBenedict\n",
      "\tJude\n",
      "\tAnabelle\n",
      "\tRendal\n",
      "\tEliza\n",
      "\tKathryn\n",
      "\tAtha\n",
      "\tMargarita\n",
      "\tThea\n",
      "Epoch 22/50\n",
      "6782/6782 [==============================] - 11s 2ms/step - loss: 0.9296\n",
      "Epoch 23/50\n",
      "6782/6782 [==============================] - 10s 1ms/step - loss: 0.9293\n",
      "----- Generating text after Epoch: 22\n",
      "\tDarla\n",
      "\tZustey\n",
      "\tEliba\n",
      "\tMarian\n",
      "\tKaliandro\n",
      "\tKolarie\n",
      "\tMarian\n",
      "\tVergil\n",
      "\tMalissie\n",
      "\tIsana\n",
      "Epoch 24/50\n",
      "6782/6782 [==============================] - 11s 2ms/step - loss: 0.9288\n",
      "Epoch 25/50\n",
      "6782/6782 [==============================] - 11s 2ms/step - loss: 0.9291\n",
      "----- Generating text after Epoch: 24\n",
      "\tDoug\n",
      "\tRoe\n",
      "\tKatelina\n",
      "\tAlvira\n",
      "\tHolley\n",
      "\tNoemic\n",
      "\tAigie\n",
      "\tFakhi\n",
      "\tHadden\n",
      "\tAlgie\n",
      "Epoch 26/50\n",
      "6782/6782 [==============================] - 12s 2ms/step - loss: 0.9279\n",
      "Epoch 27/50\n",
      "6782/6782 [==============================] - 12s 2ms/step - loss: 0.9287\n",
      "----- Generating text after Epoch: 26\n",
      "\tWavon\n",
      "\tGlanYadut\n",
      "\tJosewil\n",
      "\tHustan\n",
      "\tImma\n",
      "\tChani\n",
      "\tKiben\n",
      "\tKamila\n",
      "\tJameie\n",
      "\tDorle\n",
      "Epoch 28/50\n",
      "6782/6782 [==============================] - 10s 2ms/step - loss: 0.9278\n",
      "Epoch 29/50\n",
      "6782/6782 [==============================] - 10s 1ms/step - loss: 0.9285\n",
      "----- Generating text after Epoch: 28\n",
      "\tRawan\n",
      "\thCristin\n",
      "\tGindie\n",
      "\tCurtis\n",
      "\tKarol\n",
      "\tTarin\n",
      "\tKinbell\n",
      "\tCona\n",
      "\tAshlyn\n",
      "\tColetta\n",
      "Epoch 30/50\n",
      "6782/6782 [==============================] - 9s 1ms/step - loss: 0.9269\n",
      "Epoch 31/50\n",
      "6782/6782 [==============================] - 9s 1ms/step - loss: 0.9265\n",
      "----- Generating text after Epoch: 30\n",
      "\tDenton\n",
      "\tWaito\n",
      "\tGlada\n",
      "\tGildin\n",
      "\tKocey\n",
      "\tRegan\n",
      "\tKenneth\n",
      "\tMargene\n",
      "\tMargon\n",
      "\tDamari\n",
      "Epoch 32/50\n",
      "6782/6782 [==============================] - 9s 1ms/step - loss: 0.9265\n",
      "Epoch 33/50\n",
      "6782/6782 [==============================] - 9s 1ms/step - loss: 0.9280\n",
      "----- Generating text after Epoch: 32\n",
      "\tWilbur\n",
      "\tNordord\n",
      "\tRhianno\n",
      "\tSevani\n",
      "\tCeleste\n",
      "\tSheret\n",
      "\tMadalyn\n",
      "\tQueel\n",
      "\tIebren\n",
      "\tAdriane\n",
      "Epoch 34/50\n",
      "6782/6782 [==============================] - 12s 2ms/step - loss: 0.9267\n",
      "Epoch 35/50\n",
      "6782/6782 [==============================] - 11s 2ms/step - loss: 0.9261\n",
      "----- Generating text after Epoch: 34\n",
      "\tCynthia\n",
      "\tCamila\n",
      "\tMari\n",
      "\tValanda\n",
      "\tLuciug\n",
      "\tCarlol\n",
      "\tRudolfe\n",
      "\tArmina\n",
      "\tOrvis\n",
      "\tVersella\n",
      "Epoch 36/50\n",
      "6782/6782 [==============================] - 10s 1ms/step - loss: 0.9259\n",
      "Epoch 37/50\n",
      "6782/6782 [==============================] - 9s 1ms/step - loss: 0.9263\n",
      "----- Generating text after Epoch: 36\n",
      "\tBrandy\n",
      "\tSherrio\n",
      "\tTabecra\n",
      "\tBascie\n",
      "\tBryan\n",
      "\tAma\n",
      "\tMeagha\n",
      "\tJanell\n",
      "\tEstie\n",
      "\tJoveta\n",
      "Epoch 38/50\n",
      "6782/6782 [==============================] - 10s 1ms/step - loss: 0.9249\n",
      "Epoch 39/50\n",
      "6782/6782 [==============================] - 10s 1ms/step - loss: 0.9248\n",
      "----- Generating text after Epoch: 38\n",
      "\thritta\n",
      "\tWarrel\n",
      "\tNathanial\n",
      "\tEbbie\n",
      "\tAlpha\n",
      "\tDanya\n",
      "\tMaximil\n",
      "\tParlit\n",
      "\tIngrid\n",
      "\tDraiger\n",
      "Epoch 40/50\n",
      "6782/6782 [==============================] - 9s 1ms/step - loss: 0.9246\n",
      "Epoch 41/50\n",
      "6782/6782 [==============================] - 9s 1ms/step - loss: 0.9253\n",
      "----- Generating text after Epoch: 40\n",
      "\tTys\n",
      "\tMare\n",
      "\tJosefoni\n",
      "\tFrank\n",
      "\tOrne\n",
      "\tViolette\n",
      "\tBeean\n",
      "\tHarder\n",
      "\tVicie\n",
      "\tOle\n",
      "Epoch 42/50\n",
      "6782/6782 [==============================] - 10s 1ms/step - loss: 0.9241\n",
      "Epoch 43/50\n",
      "6782/6782 [==============================] - 9s 1ms/step - loss: 0.9247\n",
      "----- Generating text after Epoch: 42\n",
      "\tLatiana\n",
      "\tMinnie\n",
      "\tMarianne\n",
      "\tChrista\n",
      "\tHonella\n",
      "\tClete\n",
      "\tSherrie\n",
      "\tGeorgiana\n",
      "\tBrady\n",
      "\tDon\n",
      "Epoch 44/50\n",
      "6782/6782 [==============================] - 9s 1ms/step - loss: 0.9232\n",
      "Epoch 45/50\n",
      "6782/6782 [==============================] - 9s 1ms/step - loss: 0.9224\n",
      "----- Generating text after Epoch: 44\n",
      "\tLiaba\n",
      "\tBeatriz\n",
      "\tBess\n",
      "\tCarlyn\n",
      "\tJasen\n",
      "\tTheodocy\n",
      "\tKendal\n",
      "\tLejor\n",
      "\tTori\n",
      "\tTremaidre\n",
      "Epoch 46/50\n",
      "6782/6782 [==============================] - 11s 2ms/step - loss: 0.9240\n",
      "Epoch 47/50\n",
      "6782/6782 [==============================] - 13s 2ms/step - loss: 0.9221\n",
      "----- Generating text after Epoch: 46\n",
      "\tMontrell\n",
      "\tKarrie\n",
      "\tLeonald\n",
      "\tKaylah\n",
      "\tLatoria\n",
      "\tFlorinda\n",
      "\tBillie\n",
      "\tBritch\n",
      "\tPhoexie\n",
      "\tLudie\n",
      "Epoch 48/50\n",
      "6782/6782 [==============================] - 10s 1ms/step - loss: 0.9217\n",
      "Epoch 49/50\n",
      "6782/6782 [==============================] - 8s 1ms/step - loss: 0.9221\n",
      "----- Generating text after Epoch: 48\n",
      "\tLaurance\n",
      "\tVass\n",
      "\tRylee\n",
      "\tEdnah\n",
      "\tBernie\n",
      "\tIgnatie\n",
      "\tSunniah\n",
      "\tBarnue\n",
      "\tBria\n",
      "\tHamsar\n",
      "Epoch 50/50\n",
      "6782/6782 [==============================] - 10s 1ms/step - loss: 0.9222\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model with callbacks\n",
    "# fit the model\n",
    "print_callback = LambdaCallback(on_epoch_end=onend)\n",
    "Model_LSTM=model.fit(input_data, output_data, batch_size=32,epochs=50, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Once we have trained the model, we can now sample the words. For this we will use ‘\\t’ as the seed char since that is what we have used in our training data as the first char.\n",
    "\n",
    "\n",
    "Given the seed char, I will get the first predict char probabilities, using them I will sample the first predicted char, which will then act as the input. Following code illustrates this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
